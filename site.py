import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
# –£–∫–∞–∂–∏—Ç–µ URL –≤–∞—à–µ–≥–æ —Å–∞–π—Ç–∞
BASE_URL = 'https://kryptobot.net'
# –ù–∞–∑–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–∞–π—Ç–∞
OUTPUT_DIR = 'kryptobot_net_backup'
# -----------------

# –ú–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —É–∂–µ –ø–æ—Å–µ—â–µ–Ω–Ω—ã—Ö URL, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
processed_urls = set()

def download_asset(asset_url, output_folder):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª (CSS, JS, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ —Ç.–¥.) –ø–æ URL."""
    if not asset_url or not asset_url.startswith('http'):
        return None

    try:
        response = requests.get(asset_url, headers={'User-Agent': 'Mozilla/5.0'})
        response.raise_for_status()

        # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞
        parsed_url = urlparse(asset_url)
        # –£–±–∏—Ä–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π '/' –∏–∑ –ø—É—Ç–∏
        asset_path = parsed_url.path.lstrip('/')
        # –ú–æ–≥—É—Ç –±—ã—Ç—å query –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, —Å–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
        filename = os.path.join(output_folder, os.path.basename(asset_path))

        # –°–æ–∑–¥–∞–µ–º –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        os.makedirs(os.path.dirname(filename), exist_ok=True)

        with open(filename, 'wb') as f:
            f.write(response.content)
        print(f"‚úÖ –°–∫–∞—á–∞–Ω –∞—Å—Å–µ—Ç: {asset_url}")
        return os.path.relpath(filename, start=output_folder) # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å

    except requests.exceptions.RequestException as e:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –∞—Å—Å–µ—Ç {asset_url}: {e}")
        return None
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∞—Å—Å–µ—Ç–∞ {asset_url}: {e}")
        return None


def scrape_page(page_url):
    """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Å–∫–∞–Ω–∏—Ä—É–µ—Ç –∏ —Å–∫–∞—á–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –µ–µ —Ä–µ—Å—É—Ä—Å—ã."""
    if page_url in processed_urls:
        return
    processed_urls.add(page_url)

    print(f"\n Scraping: {page_url} ...")

    try:
        response = requests.get(page_url, headers={'User-Agent': 'Mozilla/5.0'})
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # --- –°–∫–∞—á–∏–≤–∞–µ–º CSS ---
        css_folder = os.path.join(OUTPUT_DIR, 'css')
        for link in soup.find_all('link', rel='stylesheet'):
            href = link.get('href')
            if href:
                css_url = urljoin(BASE_URL, href)
                download_asset(css_url, css_folder)

        # --- –°–∫–∞—á–∏–≤–∞–µ–º JS ---
        js_folder = os.path.join(OUTPUT_DIR, 'js')
        for script in soup.find_all('script', src=True):
            src = script.get('src')
            if src:
                js_url = urljoin(BASE_URL, src)
                download_asset(js_url, js_folder)

        # --- –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML —Ñ–∞–π–ª ---
        parsed_url = urlparse(page_url)
        path = parsed_url.path
        if path == '/' or not path:
            filename = os.path.join(OUTPUT_DIR, 'index.html')
        else:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –ø—É—Ç—å –≤ –∏–º—è —Ñ–∞–π–ª–∞
            filename = os.path.join(OUTPUT_DIR, path.lstrip('/'))
            if not filename.endswith('.html'):
                 filename = os.path.join(filename, 'index.html')

        os.makedirs(os.path.dirname(filename), exist_ok=True)
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(soup.prettify())
        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {filename}")


        # --- –ò—â–µ–º –¥—Ä—É–≥–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–∞–π—Ç–∞ –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è ---
        for a_tag in soup.find_all('a', href=True):
            link = a_tag.get('href')
            next_url = urljoin(BASE_URL, link)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Å—ã–ª–∫–∞ –≤–µ–¥–µ—Ç –Ω–∞ —Ç–æ—Ç –∂–µ –¥–æ–º–µ–Ω –∏ –µ—â–µ –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞
            if urlparse(next_url).netloc == urlparse(BASE_URL).netloc:
                scrape_page(next_url)

    except requests.exceptions.RequestException as e:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page_url}: {e}")


if __name__ == '__main__':
    # –°–æ–∑–¥–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

    # –ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    scrape_page(BASE_URL)
    print("\nüéâ –ì–æ—Ç–æ–≤–æ! –°–∞–π—Ç –±—ã–ª —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –≤ –ø–∞–ø–∫—É:", OUTPUT_DIR)